{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2cddb5-c7de-4c34-9c90-8b516967171d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T18:43:28.132390Z",
     "iopub.status.busy": "2025-09-17T18:43:28.132099Z",
     "iopub.status.idle": "2025-09-17T18:43:28.137507Z",
     "shell.execute_reply": "2025-09-17T18:43:28.136379Z",
     "shell.execute_reply.started": "2025-09-17T18:43:28.132368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 2.2.3 | pyarrow: 19.0.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, pyarrow as pa, pyarrow.dataset as ds\n",
    "print(\"pandas:\", pd.__version__, \"| pyarrow:\", pa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e81cf8ef-22cc-43b4-a5c8-19784c188400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T18:43:28.811839Z",
     "iopub.status.busy": "2025-09-17T18:43:28.811561Z",
     "iopub.status.idle": "2025-09-17T18:46:50.543517Z",
     "shell.execute_reply": "2025-09-17T18:46:50.542714Z",
     "shell.execute_reply.started": "2025-09-17T18:43:28.811818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 2.2.3 | pyarrow: 19.0.1\n",
      "Loaded rows: 200,000\n",
      "Training frame shape: (191517, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>ratecodeid</th>\n",
       "      <th>vendorid</th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>dolocationid</th>\n",
       "      <th>is_airport</th>\n",
       "      <th>has_congestion_fee</th>\n",
       "      <th>duration_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.60</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.533333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_distance  hour  weekday  passenger_count  payment_type  ratecodeid  \\\n",
       "0           1.60     0        2                1             1           1   \n",
       "1           0.50     0        2                1             1           1   \n",
       "2           0.60     0        2                1             1           1   \n",
       "3           0.52     0        2                3             2           1   \n",
       "4           0.66     0        2                3             2           1   \n",
       "\n",
       "   vendorid  pulocationid  dolocationid  is_airport  has_congestion_fee  \\\n",
       "0         1             0             0           0                   0   \n",
       "1         1             0             0           0                   0   \n",
       "2         1             0             0           0                   0   \n",
       "3         1             0             0           0                   0   \n",
       "4         1             0             0           0                   0   \n",
       "\n",
       "   duration_min  \n",
       "0      8.350000  \n",
       "1      2.550000  \n",
       "2      1.950000  \n",
       "3      5.566667  \n",
       "4      3.533333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val sizes: 96000 24000\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "Train: s3://nyc-taxi-poc-101199/derived/eta_poc/train/train.csv\n",
      "Val:   s3://nyc-taxi-poc-101199/derived/eta_poc/val/val.csv\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.TrainingJob.VpcConfig.Subnets\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.TrainingJob.VpcConfig.SecurityGroupIds\n",
      "2025-09-17 18:43:37 Starting - Starting the training job...\n",
      "2025-09-17 18:43:54 Starting - Preparing the instances for training...\n",
      "2025-09-17 18:44:16 Downloading - Downloading input data...\n",
      "2025-09-17 18:44:41 Downloading - Downloading the training image...\n",
      "2025-09-17 18:45:27 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-09-17 18:45:39.458 ip-10-38-202-228.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-09-17 18:45:39.525 ip-10-38-202-228.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Failed to parse hyperparameter eval_metric value mae to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] creating symlink between Path /opt/ml/input/data/train/train.csv and destination /tmp/sagemaker_xgboost_input_data/train.csv-463154431528954529\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] File path /opt/ml/input/data/validation of input files\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Making smlinks from folder /opt/ml/input/data/validation to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] creating symlink between Path /opt/ml/input/data/validation/val.csv and destination /tmp/sagemaker_xgboost_input_data/val.csv1963403810821794305\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Train matrix has 96000 rows and 11 columns\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Validation matrix has 24000 rows\u001b[0m\n",
      "\u001b[34m[2025-09-17 18:45:39.945 ip-10-38-202-228.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-09-17 18:45:39.946 ip-10-38-202-228.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-09-17 18:45:39.946 ip-10-38-202-228.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-09-17 18:45:39.946 ip-10-38-202-228.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-09-17:18:45:39:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:15.98560#011train-mae:12.05803#011validation-rmse:16.00569#011validation-mae:12.02931\u001b[0m\n",
      "\u001b[34m[2025-09-17 18:45:40.056 ip-10-38-202-228.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-09-17 18:45:40.060 ip-10-38-202-228.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:13.33524#011train-mae:9.66937#011validation-rmse:13.38543#011validation-mae:9.65336\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:11.99020#011train-mae:7.95868#011validation-rmse:12.04928#011validation-mae:7.94850\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:10.32679#011train-mae:6.57504#011validation-rmse:10.40444#011validation-mae:6.57743\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:9.60206#011train-mae:5.87985#011validation-rmse:9.69096#011validation-mae:5.89044\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:8.52799#011train-mae:5.12136#011validation-rmse:8.63600#011validation-mae:5.14342\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:7.75855#011train-mae:4.60408#011validation-rmse:7.87378#011validation-mae:4.63077\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:7.21957#011train-mae:4.26628#011validation-rmse:7.34187#011validation-mae:4.30057\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:6.85354#011train-mae:4.06396#011validation-rmse:6.98593#011validation-mae:4.10215\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:6.74280#011train-mae:4.04704#011validation-rmse:6.87722#011validation-mae:4.08740\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:6.65174#011train-mae:4.04249#011validation-rmse:6.78648#011validation-mae:4.08492\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:6.43398#011train-mae:3.92789#011validation-rmse:6.57863#011validation-mae:3.97472\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:6.30587#011train-mae:3.87097#011validation-rmse:6.45783#011validation-mae:3.91918\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:6.22093#011train-mae:3.83769#011validation-rmse:6.37928#011validation-mae:3.88733\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:6.14241#011train-mae:3.80407#011validation-rmse:6.29865#011validation-mae:3.85052\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:6.09347#011train-mae:3.78798#011validation-rmse:6.25332#011validation-mae:3.83315\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:6.08252#011train-mae:3.79903#011validation-rmse:6.24288#011validation-mae:3.84376\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:6.04133#011train-mae:3.77852#011validation-rmse:6.20531#011validation-mae:3.82398\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:6.02199#011train-mae:3.77324#011validation-rmse:6.19021#011validation-mae:3.81867\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:6.01255#011train-mae:3.77562#011validation-rmse:6.18038#011validation-mae:3.82044\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:5.99318#011train-mae:3.76948#011validation-rmse:6.16329#011validation-mae:3.81303\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:5.98221#011train-mae:3.76566#011validation-rmse:6.15739#011validation-mae:3.80986\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:5.97430#011train-mae:3.76374#011validation-rmse:6.15434#011validation-mae:3.80972\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:5.96799#011train-mae:3.76414#011validation-rmse:6.14826#011validation-mae:3.81049\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:5.95548#011train-mae:3.75680#011validation-rmse:6.14006#011validation-mae:3.80449\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:5.95212#011train-mae:3.75759#011validation-rmse:6.13675#011validation-mae:3.80538\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:5.94361#011train-mae:3.75268#011validation-rmse:6.13072#011validation-mae:3.80151\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:5.93633#011train-mae:3.74932#011validation-rmse:6.12525#011validation-mae:3.79834\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:5.93606#011train-mae:3.75043#011validation-rmse:6.12505#011validation-mae:3.79950\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:5.93074#011train-mae:3.74883#011validation-rmse:6.12401#011validation-mae:3.79930\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:5.92695#011train-mae:3.74706#011validation-rmse:6.12396#011validation-mae:3.79891\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:5.92089#011train-mae:3.74379#011validation-rmse:6.12152#011validation-mae:3.79578\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:5.91704#011train-mae:3.74165#011validation-rmse:6.12185#011validation-mae:3.79474\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:5.91497#011train-mae:3.74103#011validation-rmse:6.12126#011validation-mae:3.79498\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:5.91390#011train-mae:3.74044#011validation-rmse:6.12152#011validation-mae:3.79446\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:5.91347#011train-mae:3.74030#011validation-rmse:6.12109#011validation-mae:3.79430\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:5.90995#011train-mae:3.73820#011validation-rmse:6.12075#011validation-mae:3.79435\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:5.90474#011train-mae:3.73594#011validation-rmse:6.11979#011validation-mae:3.79345\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:5.90312#011train-mae:3.73610#011validation-rmse:6.11864#011validation-mae:3.79377\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:5.90275#011train-mae:3.73694#011validation-rmse:6.11824#011validation-mae:3.79464\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:5.89614#011train-mae:3.73202#011validation-rmse:6.11562#011validation-mae:3.79122\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:5.89452#011train-mae:3.73219#011validation-rmse:6.11483#011validation-mae:3.79200\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:5.89376#011train-mae:3.73261#011validation-rmse:6.11458#011validation-mae:3.79268\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:5.89178#011train-mae:3.73182#011validation-rmse:6.11499#011validation-mae:3.79234\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:5.88661#011train-mae:3.72958#011validation-rmse:6.11529#011validation-mae:3.79218\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:5.88306#011train-mae:3.72773#011validation-rmse:6.11443#011validation-mae:3.79224\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:5.88305#011train-mae:3.72860#011validation-rmse:6.11444#011validation-mae:3.79307\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:5.88086#011train-mae:3.72786#011validation-rmse:6.11539#011validation-mae:3.79341\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:5.87753#011train-mae:3.72696#011validation-rmse:6.11473#011validation-mae:3.79360\u001b[0m\n",
      "\u001b[34m[49]#011train-rmse:5.87391#011train-mae:3.72508#011validation-rmse:6.11542#011validation-mae:3.79353\u001b[0m\n",
      "\u001b[34m[50]#011train-rmse:5.87199#011train-mae:3.72423#011validation-rmse:6.11586#011validation-mae:3.79354\u001b[0m\n",
      "\u001b[34m[51]#011train-rmse:5.87144#011train-mae:3.72467#011validation-rmse:6.11550#011validation-mae:3.79405\u001b[0m\n",
      "\u001b[34m[52]#011train-rmse:5.86311#011train-mae:3.72193#011validation-rmse:6.11277#011validation-mae:3.79298\u001b[0m\n",
      "\u001b[34m[53]#011train-rmse:5.85981#011train-mae:3.72054#011validation-rmse:6.11247#011validation-mae:3.79241\u001b[0m\n",
      "\u001b[34m[54]#011train-rmse:5.85909#011train-mae:3.72000#011validation-rmse:6.11218#011validation-mae:3.79227\u001b[0m\n",
      "\u001b[34m[55]#011train-rmse:5.85856#011train-mae:3.72031#011validation-rmse:6.11206#011validation-mae:3.79264\u001b[0m\n",
      "\u001b[34m[56]#011train-rmse:5.85586#011train-mae:3.71823#011validation-rmse:6.11123#011validation-mae:3.79112\u001b[0m\n",
      "\u001b[34m[57]#011train-rmse:5.85367#011train-mae:3.71719#011validation-rmse:6.11193#011validation-mae:3.79121\u001b[0m\n",
      "\u001b[34m[58]#011train-rmse:5.84828#011train-mae:3.71547#011validation-rmse:6.11482#011validation-mae:3.79257\u001b[0m\n",
      "\u001b[34m[59]#011train-rmse:5.84547#011train-mae:3.71346#011validation-rmse:6.11372#011validation-mae:3.79134\u001b[0m\n",
      "\u001b[34m[60]#011train-rmse:5.84516#011train-mae:3.71335#011validation-rmse:6.11366#011validation-mae:3.79161\u001b[0m\n",
      "\u001b[34m[61]#011train-rmse:5.84444#011train-mae:3.71308#011validation-rmse:6.11362#011validation-mae:3.79178\u001b[0m\n",
      "\u001b[34m[62]#011train-rmse:5.83804#011train-mae:3.71124#011validation-rmse:6.11275#011validation-mae:3.79183\u001b[0m\n",
      "\u001b[34m[63]#011train-rmse:5.83766#011train-mae:3.71070#011validation-rmse:6.11302#011validation-mae:3.79172\u001b[0m\n",
      "\u001b[34m[64]#011train-rmse:5.83700#011train-mae:3.71125#011validation-rmse:6.11339#011validation-mae:3.79304\u001b[0m\n",
      "\u001b[34m[65]#011train-rmse:5.83396#011train-mae:3.71035#011validation-rmse:6.11557#011validation-mae:3.79380\u001b[0m\n",
      "\u001b[34m[66]#011train-rmse:5.83022#011train-mae:3.70900#011validation-rmse:6.11417#011validation-mae:3.79340\u001b[0m\n",
      "\u001b[34m[67]#011train-rmse:5.82816#011train-mae:3.70811#011validation-rmse:6.11694#011validation-mae:3.79422\u001b[0m\n",
      "\u001b[34m[68]#011train-rmse:5.82652#011train-mae:3.70748#011validation-rmse:6.11707#011validation-mae:3.79401\u001b[0m\n",
      "\u001b[34m[69]#011train-rmse:5.82272#011train-mae:3.70630#011validation-rmse:6.11645#011validation-mae:3.79407\u001b[0m\n",
      "\u001b[34m[70]#011train-rmse:5.81922#011train-mae:3.70436#011validation-rmse:6.11777#011validation-mae:3.79347\u001b[0m\n",
      "\u001b[34m[71]#011train-rmse:5.81871#011train-mae:3.70438#011validation-rmse:6.11774#011validation-mae:3.79377\u001b[0m\n",
      "\u001b[34m[72]#011train-rmse:5.81814#011train-mae:3.70410#011validation-rmse:6.11884#011validation-mae:3.79405\u001b[0m\n",
      "\u001b[34m[73]#011train-rmse:5.81638#011train-mae:3.70334#011validation-rmse:6.11959#011validation-mae:3.79464\u001b[0m\n",
      "\u001b[34m[74]#011train-rmse:5.81404#011train-mae:3.70269#011validation-rmse:6.12065#011validation-mae:3.79533\u001b[0m\n",
      "\u001b[34m[75]#011train-rmse:5.81176#011train-mae:3.70106#011validation-rmse:6.12133#011validation-mae:3.79458\u001b[0m\n",
      "\u001b[34m[76]#011train-rmse:5.80842#011train-mae:3.69988#011validation-rmse:6.12171#011validation-mae:3.79479\u001b[0m\n",
      "\u001b[34m[77]#011train-rmse:5.80395#011train-mae:3.69751#011validation-rmse:6.12077#011validation-mae:3.79368\u001b[0m\n",
      "\u001b[34m[78]#011train-rmse:5.80083#011train-mae:3.69671#011validation-rmse:6.12151#011validation-mae:3.79399\u001b[0m\n",
      "\u001b[34m[79]#011train-rmse:5.80055#011train-mae:3.69756#011validation-rmse:6.12174#011validation-mae:3.79487\u001b[0m\n",
      "\u001b[34m[80]#011train-rmse:5.80045#011train-mae:3.69740#011validation-rmse:6.12185#011validation-mae:3.79474\u001b[0m\n",
      "\u001b[34m[81]#011train-rmse:5.79785#011train-mae:3.69615#011validation-rmse:6.12215#011validation-mae:3.79474\u001b[0m\n",
      "\u001b[34m[82]#011train-rmse:5.79497#011train-mae:3.69433#011validation-rmse:6.12261#011validation-mae:3.79433\u001b[0m\n",
      "\u001b[34m[83]#011train-rmse:5.79466#011train-mae:3.69450#011validation-rmse:6.12250#011validation-mae:3.79486\u001b[0m\n",
      "\u001b[34m[84]#011train-rmse:5.79335#011train-mae:3.69406#011validation-rmse:6.12174#011validation-mae:3.79510\u001b[0m\n",
      "\u001b[34m[85]#011train-rmse:5.78997#011train-mae:3.69173#011validation-rmse:6.12191#011validation-mae:3.79471\u001b[0m\n",
      "\u001b[34m[86]#011train-rmse:5.78993#011train-mae:3.69175#011validation-rmse:6.12200#011validation-mae:3.79480\u001b[0m\n",
      "\u001b[34m[87]#011train-rmse:5.78961#011train-mae:3.69161#011validation-rmse:6.12227#011validation-mae:3.79536\u001b[0m\n",
      "\u001b[34m[88]#011train-rmse:5.78650#011train-mae:3.68859#011validation-rmse:6.12319#011validation-mae:3.79397\u001b[0m\n",
      "\u001b[34m[89]#011train-rmse:5.78194#011train-mae:3.68728#011validation-rmse:6.12206#011validation-mae:3.79372\u001b[0m\n",
      "\u001b[34m[90]#011train-rmse:5.77678#011train-mae:3.68572#011validation-rmse:6.12253#011validation-mae:3.79416\u001b[0m\n",
      "\u001b[34m[91]#011train-rmse:5.77361#011train-mae:3.68426#011validation-rmse:6.12296#011validation-mae:3.79464\u001b[0m\n",
      "\u001b[34m[92]#011train-rmse:5.77156#011train-mae:3.68268#011validation-rmse:6.12270#011validation-mae:3.79374\u001b[0m\n",
      "\u001b[34m[93]#011train-rmse:5.76722#011train-mae:3.68106#011validation-rmse:6.12282#011validation-mae:3.79440\u001b[0m\n",
      "\u001b[34m[94]#011train-rmse:5.76704#011train-mae:3.68094#011validation-rmse:6.12323#011validation-mae:3.79450\u001b[0m\n",
      "\u001b[34m[95]#011train-rmse:5.76468#011train-mae:3.67959#011validation-rmse:6.12303#011validation-mae:3.79426\u001b[0m\n",
      "\u001b[34m[96]#011train-rmse:5.76194#011train-mae:3.67849#011validation-rmse:6.12305#011validation-mae:3.79433\u001b[0m\n",
      "\u001b[34m[97]#011train-rmse:5.75737#011train-mae:3.67700#011validation-rmse:6.12113#011validation-mae:3.79412\u001b[0m\n",
      "\u001b[34m[98]#011train-rmse:5.75337#011train-mae:3.67536#011validation-rmse:6.11969#011validation-mae:3.79322\u001b[0m\n",
      "\u001b[34m[99]#011train-rmse:5.75112#011train-mae:3.67380#011validation-rmse:6.11913#011validation-mae:3.79249\u001b[0m\n",
      "\u001b[34m[100]#011train-rmse:5.74973#011train-mae:3.67291#011validation-rmse:6.11996#011validation-mae:3.79258\u001b[0m\n",
      "\u001b[34m[101]#011train-rmse:5.74631#011train-mae:3.67119#011validation-rmse:6.11966#011validation-mae:3.79230\u001b[0m\n",
      "\u001b[34m[102]#011train-rmse:5.74075#011train-mae:3.66917#011validation-rmse:6.12046#011validation-mae:3.79216\u001b[0m\n",
      "\u001b[34m[103]#011train-rmse:5.73696#011train-mae:3.66734#011validation-rmse:6.12226#011validation-mae:3.79179\u001b[0m\n",
      "\u001b[34m[104]#011train-rmse:5.73390#011train-mae:3.66670#011validation-rmse:6.12216#011validation-mae:3.79213\u001b[0m\n",
      "\u001b[34m[105]#011train-rmse:5.73372#011train-mae:3.66688#011validation-rmse:6.12213#011validation-mae:3.79256\u001b[0m\n",
      "\u001b[34m[106]#011train-rmse:5.73197#011train-mae:3.66614#011validation-rmse:6.12121#011validation-mae:3.79253\u001b[0m\n",
      "\u001b[34m[107]#011train-rmse:5.73164#011train-mae:3.66645#011validation-rmse:6.12116#011validation-mae:3.79297\u001b[0m\n",
      "\u001b[34m[108]#011train-rmse:5.73146#011train-mae:3.66687#011validation-rmse:6.12096#011validation-mae:3.79332\u001b[0m\n",
      "\u001b[34m[109]#011train-rmse:5.72986#011train-mae:3.66604#011validation-rmse:6.11980#011validation-mae:3.79261\u001b[0m\n",
      "\u001b[34m[110]#011train-rmse:5.72670#011train-mae:3.66416#011validation-rmse:6.12042#011validation-mae:3.79229\u001b[0m\n",
      "\u001b[34m[111]#011train-rmse:5.72466#011train-mae:3.66326#011validation-rmse:6.11962#011validation-mae:3.79185\u001b[0m\n",
      "\u001b[34m[112]#011train-rmse:5.72163#011train-mae:3.66310#011validation-rmse:6.11855#011validation-mae:3.79262\u001b[0m\n",
      "\u001b[34m[113]#011train-rmse:5.71833#011train-mae:3.66169#011validation-rmse:6.11847#011validation-mae:3.79209\u001b[0m\n",
      "\u001b[34m[114]#011train-rmse:5.71804#011train-mae:3.66209#011validation-rmse:6.11835#011validation-mae:3.79246\u001b[0m\n",
      "\u001b[34m[115]#011train-rmse:5.71534#011train-mae:3.66067#011validation-rmse:6.11819#011validation-mae:3.79170\u001b[0m\n",
      "\u001b[34m[116]#011train-rmse:5.71171#011train-mae:3.65961#011validation-rmse:6.11685#011validation-mae:3.79153\u001b[0m\n",
      "\u001b[34m[117]#011train-rmse:5.70852#011train-mae:3.65832#011validation-rmse:6.11587#011validation-mae:3.79118\u001b[0m\n",
      "\u001b[34m[118]#011train-rmse:5.70381#011train-mae:3.65666#011validation-rmse:6.11861#011validation-mae:3.79176\u001b[0m\n",
      "\u001b[34m[119]#011train-rmse:5.70001#011train-mae:3.65473#011validation-rmse:6.12090#011validation-mae:3.79215\u001b[0m\n",
      "\n",
      "2025-09-17 18:46:05 Uploading - Uploading generated training model\n",
      "2025-09-17 18:46:05 Completed - Training job completed\n",
      "Training seconds: 109\n",
      "Billable seconds: 109\n",
      "Training complete (managed job).\n",
      "Model artifact: s3://amazon-sagemaker-166725664013-us-east-1-40b919aa80ef/dzd_4a968141po04fb/aq0kpzrmretbtz/dev/sagemaker-xgboost-2025-09-17-18-43-32-715/output/model.tar.gz\n",
      "Using model file: /tmp/xgb_model/xgboost-model\n",
      "XGBoost model MAE (min): 3.792\n",
      "Tableau CSV written to: s3://nyc-taxi-poc-101199/derived/eta_poc/eta_predictions_for_tableau.csv\n",
      "HTTP download (if public or via signed URL): https://nyc-taxi-poc-101199.s3.amazonaws.com/derived/eta_poc/eta_predictions_for_tableau.csv\n",
      "Naive baseline MAE (min): 4.379\n",
      "Wrote errors sample to: s3://nyc-taxi-poc-101199/derived/eta_poc/eta_errors_sample.csv\n",
      "✅ All done\n"
     ]
    }
   ],
   "source": [
    "# ====================== EDIT THESE ======================\n",
    "S3_PARQUET       = \"s3://nyc-taxi-poc-101199/raw/yellow_tripdata_2025-01.parquet\"\n",
    "S3_OUTPUT_PREFIX = \"s3://nyc-taxi-poc-101199/derived/eta_poc/\"\n",
    "ROWS_TARGET      = 200_000   # how many rows to load into RAM (keep modest)\n",
    "# =======================================================\n",
    "\n",
    "import os, uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import boto3, sagemaker\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "print(\"pandas:\", pd.__version__, \"| pyarrow:\", pa.__version__)\n",
    "\n",
    "# -------------------------\n",
    "# 1) LOAD A BOUNDED CHUNK\n",
    "# -------------------------\n",
    "dataset = ds.dataset(S3_PARQUET, format=\"parquet\")\n",
    "\n",
    "need_cols = [\n",
    "    \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "    \"trip_distance\", \"passenger_count\",\n",
    "    \"payment_type\", \"ratecodeid\", \"vendorid\",\n",
    "    \"pulocationid\", \"dolocationid\",\n",
    "    \"total_amount\",               # for cleaning\n",
    "    \"airport_fee\", \"cbd_congestion_fee\"  # may be absent in some months\n",
    "]\n",
    "cols = [c for c in need_cols if c in dataset.schema.names]\n",
    "\n",
    "scanner = dataset.scanner(columns=cols, batch_size=50_000)\n",
    "batches, rows_loaded = [], 0\n",
    "for batch in scanner.to_batches():\n",
    "    batches.append(batch)\n",
    "    rows_loaded += len(batch)\n",
    "    if rows_loaded >= ROWS_TARGET:\n",
    "        break\n",
    "\n",
    "table = pa.Table.from_batches(batches)\n",
    "df = table.to_pandas()\n",
    "print(f\"Loaded rows: {len(df):,}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) FEATURE ENGINEERING\n",
    "# -------------------------\n",
    "# Timestamps\n",
    "df[\"tpep_pickup_datetime\"]  = pd.to_datetime(df[\"tpep_pickup_datetime\"],  errors=\"coerce\")\n",
    "df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df[\"tpep_dropoff_datetime\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\"])\n",
    "\n",
    "# Label (duration in minutes)\n",
    "df[\"duration_min\"] = (df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]).dt.total_seconds() / 60\n",
    "\n",
    "# Clean: keep reasonable trips\n",
    "df = df[\n",
    "    (df[\"duration_min\"].between(1, 120)) &\n",
    "    (df[\"trip_distance\"] >= 0) &\n",
    "    (df[\"total_amount\"]  >= 0)\n",
    "].copy()\n",
    "\n",
    "# Time features\n",
    "df[\"hour\"]    = df[\"tpep_pickup_datetime\"].dt.hour\n",
    "df[\"weekday\"] = df[\"tpep_pickup_datetime\"].dt.weekday\n",
    "\n",
    "# Optional flags (safe even if columns missing)\n",
    "df[\"is_airport\"] = 0\n",
    "if \"airport_fee\" in df.columns:\n",
    "    df[\"is_airport\"] = (df[\"airport_fee\"].fillna(0) > 0).astype(int)\n",
    "\n",
    "df[\"has_congestion_fee\"] = 0\n",
    "if \"cbd_congestion_fee\" in df.columns:\n",
    "    df[\"has_congestion_fee\"] = (df[\"cbd_congestion_fee\"].fillna(0) > 0).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# 3) BUILD TRAINING FRAME\n",
    "# -------------------------\n",
    "feature_cols = [\n",
    "    \"trip_distance\",\"hour\",\"weekday\",\"passenger_count\",\n",
    "    \"payment_type\",\"ratecodeid\",\"vendorid\",\"pulocationid\",\"dolocationid\",\n",
    "    \"is_airport\",\"has_congestion_fee\"\n",
    "]\n",
    "target_col = \"duration_min\"\n",
    "\n",
    "# Fill any missing feature columns with safe numeric defaults\n",
    "defaults = {\n",
    "    \"trip_distance\": 0.0, \"hour\": 12, \"weekday\": 3, \"passenger_count\": 1.0,\n",
    "    \"payment_type\": 1, \"ratecodeid\": 1, \"vendorid\": 1,\n",
    "    \"pulocationid\": 0, \"dolocationid\": 0,\n",
    "    \"is_airport\": 0, \"has_congestion_fee\": 0\n",
    "}\n",
    "for col in feature_cols:\n",
    "    if col not in df.columns:\n",
    "        df[col] = defaults[col]\n",
    "\n",
    "# Final guardrail\n",
    "df = df[\n",
    "    (df[\"duration_min\"].between(1, 120)) &\n",
    "    (df[\"trip_distance\"] >= 0)\n",
    "].copy()\n",
    "\n",
    "data = df[feature_cols + [target_col]].dropna().reset_index(drop=True)\n",
    "print(\"Training frame shape:\", data.shape)\n",
    "display(data.head())\n",
    "\n",
    "# -------------------------\n",
    "# 4) TRAIN / VALIDATION SPLIT\n",
    "# -------------------------\n",
    "# XGBoost expects: label first, no header\n",
    "cols_order = [target_col] + feature_cols\n",
    "trainable  = data[cols_order]\n",
    "\n",
    "# Keep it snappy\n",
    "sample_size = min(len(trainable), 120_000)\n",
    "sample = trainable.sample(sample_size, random_state=42)\n",
    "\n",
    "train_df, val_df = train_test_split(sample, test_size=0.2, random_state=42)\n",
    "train_path = \"train.csv\"\n",
    "val_path   = \"val.csv\"\n",
    "train_df.to_csv(train_path, index=False, header=False)\n",
    "val_df.to_csv(val_path,   index=False, header=False)\n",
    "print(\"Train/Val sizes:\", len(train_df), len(val_df))\n",
    "\n",
    "# -------------------------\n",
    "# 5) MANAGED TRAINING JOB (BUILT-IN XGBOOST)\n",
    "# -------------------------\n",
    "sm_sess = sagemaker.Session()\n",
    "role    = sagemaker.get_execution_role()\n",
    "region  = sm_sess.boto_region_name\n",
    "xgb_img = image_uris.retrieve(\"xgboost\", region=region, version=\"1.7-1\")\n",
    "\n",
    "# Upload train/val to S3 under your prefix\n",
    "def parse_s3(s3url):\n",
    "    assert s3url.startswith(\"s3://\")\n",
    "    bkt_key = s3url[5:]\n",
    "    bucket, _, key = bkt_key.partition(\"/\")\n",
    "    return bucket, key if key.endswith(\"/\") else (key + \"/\" if key else \"\")\n",
    "\n",
    "bucket, prefix = parse_s3(S3_OUTPUT_PREFIX)\n",
    "train_s3_uri = sm_sess.upload_data(path=train_path, bucket=bucket, key_prefix=prefix + \"train\")\n",
    "val_s3_uri   = sm_sess.upload_data(path=val_path,   bucket=bucket, key_prefix=prefix + \"val\")\n",
    "print(\"Train:\", train_s3_uri)\n",
    "print(\"Val:  \", val_s3_uri)\n",
    "\n",
    "# Use a small instance class (eligible for free-trial hours if your account qualifies)\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgb_img,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    sagemaker_session=sm_sess,\n",
    "    hyperparameters={\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"mae\",\n",
    "        \"max_depth\": 6,\n",
    "        \"eta\": 0.2,\n",
    "        \"subsample\": 0.8,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"num_round\": 120,\n",
    "        \"verbosity\": 1\n",
    "    },\n",
    ")\n",
    "\n",
    "estimator.fit({\n",
    "    \"train\": TrainingInput(train_s3_uri, content_type=\"text/csv\"),\n",
    "    \"validation\": TrainingInput(val_s3_uri, content_type=\"text/csv\"),\n",
    "})\n",
    "print(\"Training complete (managed job).\")\n",
    "\n",
    "# === Local predictions from the trained model artifact (no Batch Transform) ===\n",
    "import os, tarfile, boto3, numpy as np, pandas as pd\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "model_s3 = estimator.model_data  # s3://.../output/model.tar.gz\n",
    "print(\"Model artifact:\", model_s3)\n",
    "\n",
    "def _parse_s3(url):\n",
    "    rest = url[5:]; b, _, k = rest.partition(\"/\")\n",
    "    return b, k\n",
    "bucket_model, key_model = _parse_s3(model_s3)\n",
    "\n",
    "local_tar = \"/tmp/model.tar.gz\"\n",
    "s3.download_file(bucket_model, key_model, local_tar)\n",
    "\n",
    "extract_dir = \"/tmp/xgb_model\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "with tarfile.open(local_tar) as tar:\n",
    "    tar.extractall(extract_dir)\n",
    "\n",
    "def _find_model_file(root):\n",
    "    picks = []\n",
    "    for dp, _, files in os.walk(root):\n",
    "        for f in files:\n",
    "            if f in (\"xgboost-model\", \"model.xgb\") or f.endswith((\".json\", \".bin\")):\n",
    "                picks.append(os.path.join(dp, f))\n",
    "    for f in picks:\n",
    "        if os.path.basename(f) in (\"xgboost-model\", \"model.xgb\") or f.endswith(\".bin\"):\n",
    "            return f\n",
    "    return picks[0] if picks else None\n",
    "\n",
    "model_path = _find_model_file(extract_dir)\n",
    "assert model_path, f\"No model file found under {extract_dir}\"\n",
    "print(\"Using model file:\", model_path)\n",
    "\n",
    "# Load booster and predict on validation set\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"xgboost==1.7.6\"])\n",
    "    import xgboost as xgb\n",
    "\n",
    "# val_df is label-first (duration_min then features) from your code above\n",
    "y_true = val_df.iloc[:, 0].to_numpy()\n",
    "X_val  = val_df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "dm = xgb.DMatrix(X_val)\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(model_path)\n",
    "y_pred = booster.predict(dm)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae_model = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"XGBoost model MAE (min): {mae_model:0.3f}\")\n",
    "\n",
    "# Build a compact Tableau-friendly CSV with predictions and errors\n",
    "feature_cols = [\n",
    "    \"trip_distance\",\"hour\",\"weekday\",\"passenger_count\",\n",
    "    \"payment_type\",\"ratecodeid\",\"vendorid\",\"pulocationid\",\"dolocationid\",\n",
    "    \"is_airport\",\"has_congestion_fee\"\n",
    "]\n",
    "val_named = val_df.copy()\n",
    "val_named.columns = [\"duration_min\"] + feature_cols\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    \"duration_min\": y_true,\n",
    "    \"pred_eta_min\": y_pred,\n",
    "    \"abs_err\": np.abs(y_true - y_pred),\n",
    "    \"hour\": val_named[\"hour\"].to_numpy(),\n",
    "    \"trip_distance\": val_named[\"trip_distance\"].to_numpy(),\n",
    "    \"weekday\": val_named[\"weekday\"].to_numpy(),\n",
    "    \"passenger_count\": val_named[\"passenger_count\"].to_numpy(),\n",
    "    \"pulocationid\": val_named[\"pulocationid\"].to_numpy(),\n",
    "    \"dolocationid\": val_named[\"dolocationid\"].to_numpy(),\n",
    "})\n",
    "\n",
    "# Keep it light for Tableau; sample if huge\n",
    "out_sample = out.sample(min(50_000, len(out)), random_state=7)\n",
    "local_pred_csv = \"eta_predictions_for_tableau.csv\"\n",
    "out_sample.to_csv(local_pred_csv, index=False)\n",
    "\n",
    "# Upload alongside your derived outputs so you can download easily\n",
    "def _parse_prefix(s3url):\n",
    "    rest = s3url[5:]; b, _, k = rest.partition(\"/\")\n",
    "    if k and not k.endswith(\"/\"): k += \"/\"\n",
    "    return b, k\n",
    "\n",
    "bucket_out, prefix_out = _parse_prefix(S3_OUTPUT_PREFIX)\n",
    "dst_key = f\"{prefix_out}eta_predictions_for_tableau.csv\"\n",
    "s3.upload_file(local_pred_csv, bucket_out, dst_key)\n",
    "\n",
    "print(\"Tableau CSV written to:\", f\"s3://{bucket_out}/{dst_key}\")\n",
    "print(\"HTTP download (if public or via signed URL):\", f\"https://{bucket_out}.s3.amazonaws.com/{dst_key}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6) QUICK BASELINE MAE\n",
    "# -------------------------\n",
    "bin_edges = [0, 0.5, 1, 2, 5, 10, 20, np.inf]\n",
    "\n",
    "train_tmp = train_df.copy()\n",
    "train_tmp.columns = [target_col] + feature_cols\n",
    "train_tmp[\"dist_bin\"] = pd.cut(train_tmp[\"trip_distance\"], bins=bin_edges)\n",
    "med = train_tmp.groupby([\"hour\",\"dist_bin\"])[target_col].median()\n",
    "\n",
    "def naive_predict(row):\n",
    "    b = pd.cut([row[\"trip_distance\"]], bins=bin_edges)[0]\n",
    "    return med.get((row[\"hour\"], b), med.median())\n",
    "\n",
    "val_tmp = val_df.copy()\n",
    "val_tmp.columns = [target_col] + feature_cols\n",
    "y_true = val_tmp[target_col].to_numpy()\n",
    "y_pred_naive = val_tmp.apply(naive_predict, axis=1).to_numpy()\n",
    "\n",
    "mae_naive = mean_absolute_error(y_true, y_pred_naive)\n",
    "print(f\"Naive baseline MAE (min): {mae_naive:0.3f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) SAVE SMALL ERROR SAMPLE TO S3\n",
    "# -------------------------\n",
    "err = val_tmp[[target_col, \"hour\", \"trip_distance\"]].copy()\n",
    "err[\"naive_eta\"] = y_pred_naive\n",
    "err[\"abs_err\"]   = (err[target_col] - err[\"naive_eta\"]).abs()\n",
    "\n",
    "err_out = err.sample(min(5000, len(err)), random_state=7)\n",
    "local_csv = \"eta_errors_sample.csv\"\n",
    "os.makedirs(\"/tmp/eta_poc\", exist_ok=True)\n",
    "err_out.to_csv(local_csv, index=False)\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "dst_key = f\"{prefix}eta_errors_sample.csv\"\n",
    "s3.upload_file(local_csv, bucket, dst_key)\n",
    "print(\"Wrote errors sample to:\", f\"s3://{bucket}/{dst_key}\")\n",
    "print(\"✅ All done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ad6fb33-634d-40d3-9d75-cfdeab4c520a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T18:46:50.545897Z",
     "iopub.status.busy": "2025-09-17T18:46:50.545636Z",
     "iopub.status.idle": "2025-09-17T18:46:51.882303Z",
     "shell.execute_reply": "2025-09-17T18:46:51.881471Z",
     "shell.execute_reply.started": "2025-09-17T18:46:50.545876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact: s3://amazon-sagemaker-166725664013-us-east-1-40b919aa80ef/dzd_4a968141po04fb/aq0kpzrmretbtz/dev/sagemaker-xgboost-2025-09-17-18-43-32-715/output/model.tar.gz\n",
      "Using model file: /tmp/xgb_model/xgboost-model\n",
      "XGBoost model MAE (min): 3.792\n",
      "Wrote model errors sample to: s3://nyc-taxi-poc-101199/derived/eta_poc/eta_errors_model_sample.csv\n",
      "Model artifact: s3://amazon-sagemaker-166725664013-us-east-1-40b919aa80ef/dzd_4a968141po04fb/aq0kpzrmretbtz/dev/sagemaker-xgboost-2025-09-17-18-43-32-715/output/model.tar.gz\n",
      "Using model file: /tmp/xgb_model/xgboost-model\n",
      "XGBoost model MAE (min): 3.792\n",
      "Wrote model errors sample to: s3://nyc-taxi-poc-101199/derived/eta_poc/eta_errors_model_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# === Local predictions from the trained model artifact (no Batch Transform) ===\n",
    "import os, tarfile, tempfile, boto3, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Download and extract the model artifact produced by the managed training job\n",
    "s3 = boto3.client(\"s3\")\n",
    "model_s3 = estimator.model_data  # e.g., s3://.../output/model.tar.gz\n",
    "print(\"Model artifact:\", model_s3)\n",
    "\n",
    "def parse_s3(url):\n",
    "    assert url.startswith(\"s3://\")\n",
    "    rest = url[5:]\n",
    "    bkt, _, key = rest.partition(\"/\")\n",
    "    return bkt, key\n",
    "\n",
    "bkt, key = parse_s3(model_s3)\n",
    "local_tar = \"/tmp/model.tar.gz\"\n",
    "s3.download_file(bkt, key, local_tar)\n",
    "\n",
    "extract_dir = \"/tmp/xgb_model\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "with tarfile.open(local_tar) as tar:\n",
    "    tar.extractall(extract_dir)\n",
    "\n",
    "# Find the model file (name varies by container/version)\n",
    "def find_model_file(root):\n",
    "    candidates = []\n",
    "    for dp, dn, fn in os.walk(root):\n",
    "        for f in fn:\n",
    "            if f in (\"xgboost-model\", \"model.xgb\") or f.endswith((\".json\", \".bin\")):\n",
    "                candidates.append(os.path.join(dp, f))\n",
    "    # prefer non-json binary first, else json\n",
    "    for f in candidates:\n",
    "        if os.path.basename(f) in (\"xgboost-model\", \"model.xgb\") or f.endswith(\".bin\"):\n",
    "            return f\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "model_path = find_model_file(extract_dir)\n",
    "assert model_path, f\"No model file found under {extract_dir}\"\n",
    "print(\"Using model file:\", model_path)\n",
    "\n",
    "# 2) Load booster and predict on your validation features\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    # lightweight install if missing\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"xgboost==1.7.6\"])\n",
    "    import xgboost as xgb\n",
    "\n",
    "# val_df is label-first (duration_min), then features – created earlier in your script\n",
    "val_tmp = val_df.copy()\n",
    "y_true = val_tmp.iloc[:, 0].to_numpy()\n",
    "X_val  = val_tmp.iloc[:, 1:].to_numpy()\n",
    "\n",
    "dm = xgb.DMatrix(X_val)\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(model_path)\n",
    "y_pred = booster.predict(dm)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae_model = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"XGBoost model MAE (min): {mae_model:0.3f}\")\n",
    "\n",
    "# 3) Save a compact errors file for Superset\n",
    "#    (keep a couple of useful features for slicing)\n",
    "feature_cols = [\n",
    "    \"trip_distance\",\"hour\",\"weekday\",\"passenger_count\",\n",
    "    \"payment_type\",\"ratecodeid\",\"vendorid\",\"pulocationid\",\"dolocationid\",\n",
    "    \"is_airport\",\"has_congestion_fee\"\n",
    "]\n",
    "val_named = val_tmp.copy()\n",
    "val_named.columns = [\"duration_min\"] + feature_cols\n",
    "\n",
    "err = pd.DataFrame({\n",
    "    \"duration_min\": y_true,\n",
    "    \"pred_eta_min\": y_pred,\n",
    "    \"abs_err\": np.abs(y_true - y_pred),\n",
    "    \"hour\": val_named[\"hour\"].to_numpy(),\n",
    "    \"trip_distance\": val_named[\"trip_distance\"].to_numpy(),\n",
    "})\n",
    "\n",
    "err_sample = err.sample(min(5000, len(err)), random_state=7)\n",
    "local_csv = \"eta_errors_model_sample.csv\"\n",
    "err_sample.to_csv(local_csv, index=False)\n",
    "\n",
    "# Upload next to your other derived files\n",
    "def parse_prefix(s3url):\n",
    "    assert s3url.startswith(\"s3://\")\n",
    "    rest = s3url[5:]\n",
    "    bucket, _, key = rest.partition(\"/\")\n",
    "    if key and not key.endswith(\"/\"): key += \"/\"\n",
    "    return bucket, key\n",
    "\n",
    "bucket, prefix = parse_prefix(S3_OUTPUT_PREFIX)\n",
    "dst_key = f\"{prefix}eta_errors_model_sample.csv\"\n",
    "s3.upload_file(local_csv, bucket, dst_key)\n",
    "print(\"Wrote model errors sample to:\", f\"s3://{bucket}/{dst_key}\")\n",
    "# === Local predictions from the trained model artifact (no Batch Transform) ===\n",
    "import os, tarfile, tempfile, boto3, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Download and extract the model artifact produced by the managed training job\n",
    "s3 = boto3.client(\"s3\")\n",
    "model_s3 = estimator.model_data  # e.g., s3://.../output/model.tar.gz\n",
    "print(\"Model artifact:\", model_s3)\n",
    "\n",
    "def parse_s3(url):\n",
    "    assert url.startswith(\"s3://\")\n",
    "    rest = url[5:]\n",
    "    bkt, _, key = rest.partition(\"/\")\n",
    "    return bkt, key\n",
    "\n",
    "bkt, key = parse_s3(model_s3)\n",
    "local_tar = \"/tmp/model.tar.gz\"\n",
    "s3.download_file(bkt, key, local_tar)\n",
    "\n",
    "extract_dir = \"/tmp/xgb_model\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "with tarfile.open(local_tar) as tar:\n",
    "    tar.extractall(extract_dir)\n",
    "\n",
    "# Find the model file (name varies by container/version)\n",
    "def find_model_file(root):\n",
    "    candidates = []\n",
    "    for dp, dn, fn in os.walk(root):\n",
    "        for f in fn:\n",
    "            if f in (\"xgboost-model\", \"model.xgb\") or f.endswith((\".json\", \".bin\")):\n",
    "                candidates.append(os.path.join(dp, f))\n",
    "    # prefer non-json binary first, else json\n",
    "    for f in candidates:\n",
    "        if os.path.basename(f) in (\"xgboost-model\", \"model.xgb\") or f.endswith(\".bin\"):\n",
    "            return f\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "model_path = find_model_file(extract_dir)\n",
    "assert model_path, f\"No model file found under {extract_dir}\"\n",
    "print(\"Using model file:\", model_path)\n",
    "\n",
    "# 2) Load booster and predict on your validation features\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    # lightweight install if missing\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"xgboost==1.7.6\"])\n",
    "    import xgboost as xgb\n",
    "\n",
    "# val_df is label-first (duration_min), then features – created earlier in your script\n",
    "val_tmp = val_df.copy()\n",
    "y_true = val_tmp.iloc[:, 0].to_numpy()\n",
    "X_val  = val_tmp.iloc[:, 1:].to_numpy()\n",
    "\n",
    "dm = xgb.DMatrix(X_val)\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(model_path)\n",
    "y_pred = booster.predict(dm)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae_model = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"XGBoost model MAE (min): {mae_model:0.3f}\")\n",
    "\n",
    "# 3) Save a compact errors file for Superset\n",
    "#    (keep a couple of useful features for slicing)\n",
    "feature_cols = [\n",
    "    \"trip_distance\",\"hour\",\"weekday\",\"passenger_count\",\n",
    "    \"payment_type\",\"ratecodeid\",\"vendorid\",\"pulocationid\",\"dolocationid\",\n",
    "    \"is_airport\",\"has_congestion_fee\"\n",
    "]\n",
    "val_named = val_tmp.copy()\n",
    "val_named.columns = [\"duration_min\"] + feature_cols\n",
    "\n",
    "err = pd.DataFrame({\n",
    "    \"duration_min\": y_true,\n",
    "    \"pred_eta_min\": y_pred,\n",
    "    \"abs_err\": np.abs(y_true - y_pred),\n",
    "    \"hour\": val_named[\"hour\"].to_numpy(),\n",
    "    \"trip_distance\": val_named[\"trip_distance\"].to_numpy(),\n",
    "})\n",
    "\n",
    "err_sample = err.sample(min(5000, len(err)), random_state=7)\n",
    "local_csv = \"eta_errors_model_sample.csv\"\n",
    "err_sample.to_csv(local_csv, index=False)\n",
    "\n",
    "# Upload next to your other derived files\n",
    "def parse_prefix(s3url):\n",
    "    assert s3url.startswith(\"s3://\")\n",
    "    rest = s3url[5:]\n",
    "    bucket, _, key = rest.partition(\"/\")\n",
    "    if key and not key.endswith(\"/\"): key += \"/\"\n",
    "    return bucket, key\n",
    "\n",
    "bucket, prefix = parse_prefix(S3_OUTPUT_PREFIX)\n",
    "dst_key = f\"{prefix}eta_errors_model_sample.csv\"\n",
    "s3.upload_file(local_csv, bucket, dst_key)\n",
    "print(\"Wrote model errors sample to:\", f\"s3://{bucket}/{dst_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a06a3-9bd7-4ffa-a2e5-5d3ee7f9e5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
